{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "-FrAeMLUXtdK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kEVPMcrM0uui",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56b66c0a-e6ef-49c4-f0d2-a15ad4ff2575"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests (from transformers)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, requests, pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-16.1.0 requests-2.32.3 xxhash-3.4.1\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for preprocessor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.0/233.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: holidays in /usr/local/lib/python3.10/dist-packages (0.52)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from holidays) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->holidays) (1.16.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n"
          ]
        }
      ],
      "source": [
        "# PyTprch and Transformers installation and import\n",
        "# These are preinstalled so you will get \"Requirement already satisfied\" but nevertheless, it is required before you import the relevant packages.\n",
        "# (not needed for the basic classificaiton models)\n",
        "\n",
        "! pip install transformers datasets\n",
        "! pip3 install torch\n",
        "\n",
        "# Installing additional libraries for text preprocessing\n",
        "!pip install -q preprocessor\n",
        "!pip install -q contractions\n",
        "!pip install -q optuna\n",
        "!pip install holidays\n",
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import holidays\n",
        "import urllib.request\n",
        "import csv\n",
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Google Drive\n",
        "import gdown\n",
        "\n",
        "# Text Preprocessing\n",
        "import re\n",
        "import preprocessor\n",
        "import contractions\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "import nltk, scipy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Feature Vectorization\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Optimization\n",
        "import optuna\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.pipeline import FeatureUnion, Pipeline\n",
        "from sklearn.metrics import f1_score, classification_report, make_scorer\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='optuna.distributions')\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# FFNN\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Transformer\n",
        "from scipy.special import softmax\n",
        "# from transformers import AutoModelForSequenceClassification\n",
        "# from transformers import TFAutoModelForSequenceClassification\n",
        "# from transformers import AutoTokenizer\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification"
      ],
      "metadata": {
        "id": "tkS9XJHo1q9j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10988cb6-dea2-48e8-959d-d1bff07b3e1d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algorithmic approaches:**\n",
        "\n",
        "\n",
        "\n",
        "1. [sklearn.linear_model.LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)\n",
        "2. [sklearn.svm.SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) (try both linear and nonlinear kernels!)\n",
        "3. FFNN classifier - You should use  the PyTorch library to build a FFNN classifier (with at least one hidden layer) to achieve the classification. Feel free to experiment with the number of layers ([a simple tutorial for FFNN with PyTorch](https://medium.com/biaslyai/pytorch-introduction-to-neural-network-feedforward-neural-network-model-e7231cff47cb)).\n",
        "4. A fourth classifier of choice (neural or not). You are encouraged to experiment with classifiers that allow combining different types of features (e.g. number of capitalized words, time of tweeting, etc.)\n",
        "5. A fifth classifier of your choice  (this should be neural -  RNN, or transformer-based) - feel free to experiment.\n",
        "\n"
      ],
      "metadata": {
        "id": "Qy428MPY1r__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support Code"
      ],
      "metadata": {
        "id": "c_ooYiYL427e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Preprocess\n",
        "\n",
        "**Our data cleaning process handles a few modifications:**\n",
        "1. Define regular expressions for different unwanted patterns and normalize them.\n",
        "\n",
        "2. Constructions are expanded to the full shape of the word and other unwanted chars are removed.\n",
        "\n",
        "3. Cleans the text to retain only alphanumeric characters and common punctuation.\n",
        "\n",
        "4. Converts text to lowercase and removes stop words (user's choice).\n",
        "\n",
        "5. Lemmatization\n",
        "\n",
        "6. Removal of rows with missing values."
      ],
      "metadata": {
        "id": "CcfGMJ3k5NUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Define regex patterns\n",
        "re_url = re.compile(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\")\n",
        "re_html = re.compile(r'<[^<]+?>')\n",
        "re_date = re.compile(r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b|\\b\\d{2,4}[/-]\\d{1,2}[/-]\\d{1,2}\\b')\n",
        "re_decimal = re.compile(r'\\b\\d+\\.\\d+%?\\b')\n",
        "re_number_percent = re.compile(r'\\b\\d+%?\\b')\n",
        "re_char = re.compile(r'[^0-9a-zA-Z\\s?!.,:\\'\\\"//]+')\n",
        "\n",
        "# Function for cleaning text\n",
        "def PreProcessText(text, lower=False, remove_stopwords=True):\n",
        "    # Replacement mappings for common misencoded characters\n",
        "    replacements = {\n",
        "        '“': '\"', '”': '\"', '‘': \"'\", '’': \"'\",\n",
        "        'â\\x80\\x9c': '\"', 'â\\x80\\x9d': '\"', 'â\\x80\\x99': \"'\"\n",
        "    }\n",
        "\n",
        "    for bad_char, good_char in replacements.items():\n",
        "        text = text.replace(bad_char, good_char)\n",
        "\n",
        "    text = contractions.fix(text)  # Expand contractions\n",
        "    text = re_html.sub('', text)  # Remove HTML tags\n",
        "    text = re_url.sub('[URL]', text)  # Replace URLs with placeholder\n",
        "    text = re_date.sub('[DATE]', text)  # Replace dates with placeholder\n",
        "    text = re_decimal.sub('[DECIMAL]', text)  # Replace decimal numbers with placeholder\n",
        "    text = re_number_percent.sub('[NUM]', text)  # Replace whole numbers with placeholder\n",
        "    text = re_char.sub(\"\", text)  # Remove non-alphanumeric characters\n",
        "\n",
        "    words = text.split()\n",
        "    placeholders = ['[URL]', '[DATE]', '[DECIMAL]', '[NUM]']\n",
        "\n",
        "    if lower:\n",
        "        words = [word.lower() for word in words if word not in placeholders]  # Convert to lowercase\n",
        "\n",
        "    if remove_stopwords:\n",
        "        words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
        "\n",
        "    words = [lemmatizer.lemmatize(word) if word not in placeholders else word for word in words]  # Lemmatize words\n",
        "\n",
        "    text = ' '.join(words)  # Join words back into a single string\n",
        "\n",
        "    return text\n",
        "\n",
        "def average_timestamp(timestamps, format='%Y-%m-%d %H:%M:%S'):\n",
        "    valid_timestamps = [datetime.strptime(ts, format) for ts in timestamps if is_valid_timestamp(ts, format)]\n",
        "    if not valid_timestamps:\n",
        "        return datetime.now().strftime(format)\n",
        "    avg_timestamp = sum(map(datetime.timestamp, valid_timestamps)) / len(valid_timestamps)\n",
        "    return datetime.fromtimestamp(avg_timestamp).strftime(format)\n",
        "\n",
        "\n",
        "def is_valid_timestamp(timestamp, format='%Y-%m-%d %H:%M:%S'):\n",
        "    try:\n",
        "        datetime.strptime(timestamp, format)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False"
      ],
      "metadata": {
        "id": "lE5VDkHc46fL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add Classification to Train"
      ],
      "metadata": {
        "id": "9fbKMPXv5tZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_tweet(device, username, timestamp):\n",
        "    \"\"\"\n",
        "    Classifies a tweet as being from Trump or his staff based on the device, username, and timestamp.\n",
        "\n",
        "    Parameters:\n",
        "    device (str): The device used to send the tweet.\n",
        "    username (str): The username who sent the tweet.\n",
        "    timestamp (str): The timestamp of the tweet.\n",
        "\n",
        "    Returns:\n",
        "    int: 1 if the tweet is classified as being from Trump, otherwise 0.\n",
        "    None: if there is an error in classification.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        trump_username = 'realDonaldTrump'\n",
        "        trump_cutoff_date = datetime.strptime('2017-04-01', '%Y-%m-%d')\n",
        "\n",
        "        tweet_date = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')  # Adjust the format as per your data\n",
        "\n",
        "        if username.lower() == trump_username.lower() and 'android' in device.lower() and tweet_date < trump_cutoff_date:\n",
        "            return 0\n",
        "        elif tweet_date >= trump_cutoff_date: # Can't tell anymore if trump or no\n",
        "            return None\n",
        "        else:\n",
        "            return 1\n",
        "    except Exception as e:\n",
        "        # Log the error if needed\n",
        "        return None"
      ],
      "metadata": {
        "id": "GVeHBwju5yk0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a complex features vector\n",
        "\n",
        "This vector is meant do dinamically feed different feature vector into different models for the optimization process, based on their pre-tested method.\n",
        "\n",
        "This will allow combination of independent features, TF-IDF and Word2Vec as model input."
      ],
      "metadata": {
        "id": "Ew9pM8gQ6ITG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureVectorizer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, vector_types=['tfidf', 'word2vec', 'additional'], country='US', word2vec_size=100, tfidf_max_features=5000):\n",
        "        self.vector_types = vector_types\n",
        "        self.country = country\n",
        "        self.word2vec_size = word2vec_size\n",
        "        self.tfidf_max_features = tfidf_max_features\n",
        "        self.additional_features = AdditionalFeatures(country)\n",
        "        self.word2vec_vectorizer = Word2VecVectorizer(size=word2vec_size)\n",
        "        self.tfidf_vectorizer = TfidfVectorizer(max_features=tfidf_max_features)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        if 'tfidf' in self.vector_types:\n",
        "            self.tfidf_vectorizer.fit(X['cleaned text'])\n",
        "        if 'word2vec' in self.vector_types:\n",
        "            self.word2vec_vectorizer.fit(X)\n",
        "        if 'additional' in self.vector_types:\n",
        "            self.additional_features.fit(X)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        features = []\n",
        "        if 'tfidf' in self.vector_types:\n",
        "            tfidf_features = self.tfidf_vectorizer.transform(X['cleaned text']).toarray()\n",
        "            features.append(tfidf_features)\n",
        "        if 'word2vec' in self.vector_types:\n",
        "            word2vec_features = self.word2vec_vectorizer.transform(X)\n",
        "            features.append(word2vec_features)\n",
        "        if 'additional' in self.vector_types:\n",
        "            additional_features = self.additional_features.transform(X)\n",
        "            features.append(additional_features)\n",
        "\n",
        "        # Concatenate all feature vectors\n",
        "        combined_features = np.hstack(features)\n",
        "        return combined_features\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        return self.fit(X, y).transform(X)\n",
        "\n",
        "class AdditionalFeatures(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, country='US'):\n",
        "        self.holiday_calendar = holidays.CountryHoliday(country)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([self.extract_features(text, timestamp) for text, timestamp in zip(X['tweet text'], X['time stamp'])])\n",
        "\n",
        "    def extract_features(self, text, timestamp):\n",
        "        # Initialize all feature values to 0\n",
        "        num_capitalized = 0\n",
        "        num_words = 0\n",
        "        num_exclamations = 0\n",
        "        num_questions = 0\n",
        "        hour_of_day = 0\n",
        "        day_of_week = 0\n",
        "        month = 0\n",
        "        year = 0\n",
        "        is_holiday = 0\n",
        "\n",
        "        try:\n",
        "            num_capitalized = sum(1 for c in text if c.isupper())\n",
        "            num_words = len(text.split())\n",
        "            num_exclamations = text.count('!')\n",
        "            num_questions = text.count('?')\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing text: {text}. Error: {e}\")\n",
        "\n",
        "        try:\n",
        "            dt = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n",
        "            hour_of_day = dt.hour\n",
        "            day_of_week = dt.weekday()\n",
        "            month = dt.month\n",
        "            year = dt.year\n",
        "            is_holiday = 1 if dt in self.holiday_calendar else 0\n",
        "        except ValueError as ve:\n",
        "            print(f\"Error processing timestamp: {timestamp}. Error: {ve}\")\n",
        "\n",
        "        return [num_capitalized, num_words, num_exclamations, num_questions, hour_of_day, day_of_week, month, year, is_holiday]\n",
        "\n",
        "class Word2VecVectorizer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, size=100, min_count=1):\n",
        "        self.size = size\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        sentences = [text.split() for text in X['cleaned text']]\n",
        "        self.model = Word2Vec(sentences, vector_size=self.size, min_count=self.min_count)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([self.vectorize(text) for text in X['cleaned text']])\n",
        "\n",
        "    def vectorize(self, text):\n",
        "        words = text.split()\n",
        "        word_vecs = [self.model.wv[word] for word in words if word in self.model.wv]\n",
        "        if len(word_vecs) == 0:\n",
        "            return np.zeros(self.size)\n",
        "        return np.mean(word_vecs, axis=0)"
      ],
      "metadata": {
        "id": "CGMF41vV6I9J"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self Developed Models"
      ],
      "metadata": {
        "id": "XBaWjkWZ8K6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiLayerFFNN(nn.Module):\n",
        "    '''\n",
        "    Multilayer FFNN\n",
        "    '''\n",
        "    def __init__(self, input_dim, hidden_dims, output_dim=2, dropout=0.5):\n",
        "        super(MultiLayerFFNN, self).__init__()\n",
        "        layers = []\n",
        "        current_dim = input_dim\n",
        "\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.append(nn.Linear(current_dim, hidden_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "            current_dim = hidden_dim\n",
        "\n",
        "        layers.append(nn.Linear(current_dim, output_dim))\n",
        "        layers.append(nn.LogSoftmax(dim=1))\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def predict(self, input_data):\n",
        "        self.eval()  # Set the model to evaluation mode\n",
        "        with torch.no_grad():  # Disable gradient calculation\n",
        "            outputs = self(input_data)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "        return predicted\n",
        "\n",
        "\n",
        "class PyTorchClassifier(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, hidden_dims=None, output_dim=2, dropout=0.3, learning_rate=0.001, print=False):\n",
        "        self.hidden_dims = hidden_dims if hidden_dims is not None else [50, 50]\n",
        "        self.output_dim = output_dim\n",
        "        self.dropout = dropout\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = len(self.hidden_dims) * 50\n",
        "        self.model = None\n",
        "        self.criterion = nn.NLLLoss()\n",
        "        self.optimizer = None\n",
        "        self.print = print\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        for param, value in params.items():\n",
        "            setattr(self, param, value)\n",
        "        if 'hidden_dims' in params:\n",
        "            self.num_epochs = len(self.hidden_dims) * 50\n",
        "        return self\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        input_dim = X.shape[1]\n",
        "        self.model = MultiLayerFFNN(input_dim, self.hidden_dims, self.output_dim, self.dropout)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "        y_tensor = torch.tensor(y.to_numpy(), dtype=torch.long)\n",
        "\n",
        "        dataset = TensorDataset(X_tensor, y_tensor)\n",
        "        train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            self.model.train()\n",
        "            running_loss = 0.0\n",
        "            for inputs, labels in train_loader:\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "            self.print and print(f\"Epoch [{epoch+1}/{self.num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "        predictions = self.model.predict(X_tensor)\n",
        "        return predictions.numpy()"
      ],
      "metadata": {
        "id": "EwMmzbWq8Nrf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Predict with Pre-Trained BERT"
      ],
      "metadata": {
        "id": "c8PKS9fDJ0xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_and_tokenizer(model_url, model_path):\n",
        "    \"\"\"\n",
        "    Load the DistilBERT model and tokenizer from the specified path.\n",
        "\n",
        "    Args:\n",
        "      model_url (str): The link for the model itself in google drive\n",
        "      model_path (str): Path to the directory containing model files.\n",
        "\n",
        "    Returns:\n",
        "      model: The loaded DistilBERT model.\n",
        "      tokenizer: The loaded DistilBERT tokenizer.\n",
        "    \"\"\"\n",
        "\n",
        "    def download_model_files(url, output_path):\n",
        "      \"\"\"\n",
        "      Download and extract model files from a URL.\n",
        "\n",
        "      Args:\n",
        "      url (str): URL to the zip file containing model files.\n",
        "      output_path (str): Path to extract the downloaded zip file.\n",
        "      \"\"\"\n",
        "\n",
        "      # Ensure the output directory exists\n",
        "      if not os.path.exists(output_path):\n",
        "          os.makedirs(output_path)\n",
        "\n",
        "      zip_path = os.path.join(output_path, 'model.zip')\n",
        "\n",
        "      # Download the zip file\n",
        "      gdown.download(url, zip_path, quiet=False)\n",
        "\n",
        "      # Extract the zip file\n",
        "      with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "          zip_ref.extractall(output_path)\n",
        "\n",
        "      # Remove the zip file\n",
        "      os.remove(zip_path)\n",
        "\n",
        "    # Download and extract model files\n",
        "    download_model_files(model_url, model_path)\n",
        "\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
        "    model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "Sh-ZoD76LZe2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_predict(model, tokenizer, df, column_name = 'tweet text', batch_size = 128):\n",
        "    \"\"\"\n",
        "    Predict the labels for the tweet texts in the specified column of the DataFrame in batches.\n",
        "    Uses the trained model and tokenizer to predict the labels for the given df, based on column name.\n",
        "\n",
        "    Args:\n",
        "    model: The DistilBERT model.\n",
        "    tokenizer: The DistilBERT tokenizer.\n",
        "    df (pd.DataFrame): The DataFrame containing the tweet texts.\n",
        "    column_name (str): The name of the column containing tweet texts.\n",
        "    batch_size (int): The number of samples per batch.\n",
        "\n",
        "    Returns:\n",
        "    List[int]: The list of predicted labels.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    for start in range(0, len(df), batch_size):\n",
        "        end = min(start + batch_size, len(df))\n",
        "        batch_texts = df[column_name][start:end].tolist()\n",
        "        tokens = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "        # Make Predictions\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**tokens)\n",
        "            batch_predictions = torch.argmax(outputs.logits, dim=1)\n",
        "            predictions.extend(batch_predictions.numpy().tolist())\n",
        "\n",
        "    return pd.Series(predictions)"
      ],
      "metadata": {
        "id": "EfdqfOX9O5Ch"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-Calculated Best Parameters"
      ],
      "metadata": {
        "id": "oOXeZoE6Vg-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameters = {\n",
        "    1: {\n",
        "        'model_name': 'LogisticRegression',\n",
        "        'vector': ['tfidf'],\n",
        "        'best_hyperparams': {'C': 4.008932259744248}\n",
        "    },\n",
        "    2: {\n",
        "        'model_name': 'SVC',\n",
        "        'vector': ['tfidf', 'additional'],\n",
        "        'best_hyperparams': {\n",
        "            'C': 0.8412136628232837,\n",
        "            'kernel': 'linear'\n",
        "        }\n",
        "    },\n",
        "    3: {\n",
        "        'model_name': 'XGBClassifier',\n",
        "        'vector': ['tfidf', 'additional'],\n",
        "        'best_hyperparams': {\n",
        "            'n_estimators': 82,\n",
        "            'learning_rate': 0.1877546968777687,\n",
        "            'max_depth': 7,\n",
        "            'colsample_bytree': 0.895288061016391,\n",
        "            'subsample': 0.7014186462764306,\n",
        "            'reg_alpha': 0.51961722082151,\n",
        "            'reg_lambda': 1.1930319472458693e-05\n",
        "        }\n",
        "    },\n",
        "    4: {\n",
        "        'model_name': 'FFNN',\n",
        "        'vector': ['tfidf'],\n",
        "        'best_hyperparams': {\n",
        "            'hidden': [50]\n",
        "        }\n",
        "    },\n",
        "    5: {\n",
        "        'model_name': 'DistilBERT',\n",
        "        'vector': None,\n",
        "        'best_hyperparams': None\n",
        "    }\n",
        "}\n",
        "\n",
        "# DistilBERT\n",
        "model_url = 'https://drive.google.com/uc?export=download&id=1UXjztbH_z-dpLmlNjZuSQyUnNIyUGnqt'\n",
        "model_path = '/content/distilbert_trump_classifier'\n",
        "\n",
        "# Initialize Models\n",
        "models = {\n",
        "    1: LogisticRegression(max_iter=1000, **hyperparameters[1]['best_hyperparams']),\n",
        "    2: SVC(**hyperparameters[2]['best_hyperparams']),\n",
        "    3: XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', **hyperparameters[3]['best_hyperparams']),\n",
        "    4: PyTorchClassifier(hidden_dims=hyperparameters[4]['best_hyperparams']['hidden']),\n",
        "    5: load_model_and_tokenizer(model_url, model_path)  # model, tokenizer\n",
        "}"
      ],
      "metadata": {
        "id": "B2G1kdNnXzAE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3e5b904-f210-4c6f-dac9-6622208b819a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?export=download&id=1UXjztbH_z-dpLmlNjZuSQyUnNIyUGnqt\n",
            "From (redirected): https://drive.google.com/uc?export=download&id=1UXjztbH_z-dpLmlNjZuSQyUnNIyUGnqt&confirm=t&uuid=a1e0549c-fc3f-44f6-a945-c8f3fd282587\n",
            "To: /content/distilbert_trump_classifier/model.zip\n",
            "100%|██████████| 244M/244M [00:04<00:00, 57.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# API"
      ],
      "metadata": {
        "id": "UthKh8-950lU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_pipeline(alg, train_fn = 'trump_train.tsv', test = True):\n",
        "  \"\"\"Returns a trained model given the specific task and algorithm.\n",
        "      The pipeline should include all necessary steps that are needed for the\n",
        "      specified algoritm (preprocessing, normalization, feature extraction - depending\n",
        "      on your choice and decisions). Obviously, it is advised to implement the pipeline\n",
        "      through a sequence of function calls.\n",
        "\n",
        "    Args:\n",
        "\n",
        "        alg (int): an integer between 1-5, indicating the algorithmic approach as\n",
        "                    specified above (1: logistic regression, 2: svm, 3:FFNN, etc.).\n",
        "        train_ fn (str): full path to the file containing the training data.\n",
        "        test (bool): Train the model on subset of the data and show classification report. If False - train on all data.\n",
        "\n",
        "    Returns:\n",
        "      The trained model as a pipeline object\n",
        "  \"\"\"\n",
        "  # Define the column names\n",
        "  column_names = ['tweet id', 'user handle', 'tweet text', 'time stamp', 'device']\n",
        "\n",
        "  # Load the TSV file into a DataFrame with specified column names\n",
        "  df = pd.read_csv(train_fn, sep='\\t', names=column_names, header=None)\n",
        "\n",
        "  # Process the Text\n",
        "  print('Pre-Process Data')\n",
        "  df[\"cleaned text\"] = df[\"tweet text\"].progress_apply(PreProcessText)\n",
        "\n",
        "  avg_timestamp = average_timestamp(df['time stamp'])\n",
        "  df[\"time stamp\"] = df[\"time stamp\"].apply(lambda ts: ts if is_valid_timestamp(ts) else avg_timestamp)\n",
        "\n",
        "  # Apply the classification function to the DataFrame\n",
        "  df['notTrump?'] = df.apply(lambda row: classify_tweet(row['device'], row['user handle'], row['time stamp']), axis=1)\n",
        "  df.dropna(inplace=True)\n",
        "\n",
        "  # Split the data into training, validation sets\n",
        "  # Model's will be optimized (optuna) on validation set, then assessd using cross validation on the train.\n",
        "  X = df.drop(columns=['notTrump?']).copy()\n",
        "  y = df['notTrump?'].copy()\n",
        "\n",
        "  if test:\n",
        "    X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  print('Fit model to Best Hyperparameters')\n",
        "  if alg < 5:\n",
        "    # Create a pipeline with the custom feature vectorizer and the classifier\n",
        "    model, vector = models[alg], hyperparameters[alg]['vector']\n",
        "    pipeline = Pipeline([\n",
        "        ('features', FeatureVectorizer(vector_types=vector, country='US')),\n",
        "        ('classifier', model)\n",
        "    ])\n",
        "\n",
        "    # Fit the pipeline on the entire training data\n",
        "    pipeline.fit(X, y)\n",
        "  else:\n",
        "    # Load the DistilBERT model and tokenizer\n",
        "    pipeline = models[alg]\n",
        "    model, tokenizer = pipeline\n",
        "\n",
        "  if test:\n",
        "    if alg < 5:\n",
        "      y_pred = pipeline.predict(X_test)\n",
        "    else:\n",
        "      y_pred = bert_predict(model, tokenizer, X_test, column_name='tweet text')\n",
        "\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "  return pipeline"
      ],
      "metadata": {
        "id": "Dl2qo-UY2BNX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrain_best_model():\n",
        "  \"\"\" Retrains and returns the best performing model for the specified task. The\n",
        "      function uses the hard coded settings you have found to work best for each\n",
        "      of the tasks.\n",
        "\n",
        "      Args:\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Create a pipeline with the custom feature vectorizer and the classifier\n",
        "  m = training_pipeline(3, train_fn, test = False)\n",
        "\n",
        "  return m"
      ],
      "metadata": {
        "id": "fXYZRHxK2MMu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(pipeline, fn):\n",
        "  \"\"\" Returns a list of 0s and 1s, corresponding to the lines in the specified file.\n",
        "\n",
        "    Args:\n",
        "      pipeline: the trained model and process to be used.\n",
        "      fn: the full path to a file in the same format as the test set we have proveded.\n",
        "  \"\"\"\n",
        "\n",
        "  #TODO\n",
        "  print('Pre-Process Data')\n",
        "  # Define the column names\n",
        "  column_names = ['user handle', 'tweet text', 'time stamp']\n",
        "\n",
        "  # Load the TSV file into a DataFrame with specified column names\n",
        "  data = pd.read_csv(fn, sep='\\t', names=column_names, header=None)\n",
        "\n",
        "  # Process the Text\n",
        "  data[\"cleaned text\"] = data[\"tweet text\"].progress_apply(PreProcessText)\n",
        "  avg_timestamp = average_timestamp(data['time stamp'])\n",
        "  data[\"time stamp\"] = data[\"time stamp\"].apply(lambda ts: ts if is_valid_timestamp(ts) else avg_timestamp)\n",
        "\n",
        "  print('Predict')\n",
        "  if isinstance(pipeline, Pipeline): # either a sci-kit pipline object or a trained BERT model\n",
        "    predictions = pipeline.predict(data)\n",
        "  else:\n",
        "    predictions = bert_predict(pipeline[0], pipeline[1], data, column_name='tweet text')\n",
        "\n",
        "  return predictions.tolist()"
      ],
      "metadata": {
        "id": "L6L0ZFSQ2Wjf"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def who_am_i():  # this is not a class method\n",
        "    \"\"\"Returns a ductionary with your name, id number and email. keys=['name', 'id','email']\n",
        "        Make sure you return your own info!\n",
        "    \"\"\"\n",
        "    return {'name': ['Shahar Oded', 'Omri Haller'], 'id': ['208388918', '208524413'], 'email': ['odedshah@post.bgu.ac.il', 'haller@post.bgu.ac.il']}"
      ],
      "metadata": {
        "id": "lXtCY3Mc2c4q"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Playground"
      ],
      "metadata": {
        "id": "CL1jT0c9XPcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main\n",
        "train_fn = '/content/trump_train.tsv'\n",
        "test_fn = '/content/trump_tweets_test_a.tsv'\n",
        "\n",
        "model = training_pipeline(3, train_fn, test = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b80jxOEOQuy-",
        "outputId": "ab4c5513-d92a-4910-e1d3-70c516989b20"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-Process Data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3156/3156 [00:00<00:00, 9351.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fit model to Best Hyperparameters\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.87      0.94      0.91       376\n",
            "         1.0       0.90      0.80      0.85       255\n",
            "\n",
            "    accuracy                           0.88       631\n",
            "   macro avg       0.89      0.87      0.88       631\n",
            "weighted avg       0.88      0.88      0.88       631\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = retrain_best_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TxfDZWtZOJB",
        "outputId": "be265ad7-2744-41f8-9e5d-ac6d8bb7079c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-Process Data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3156/3156 [00:00<00:00, 9247.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fit model to Best Hyperparameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = predict(model, test_fn)\n",
        "' '.join([str(pred) for pred in res])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "gLG3npNXcGoR",
        "outputId": "836f6070-b36c-44d2-85b5-19d60e02bc00"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-Process Data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 158/158 [00:00<00:00, 4514.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predict\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ]
}